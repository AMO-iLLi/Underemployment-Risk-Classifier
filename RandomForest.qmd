---
title: "RandomForest"
format: html
---

# Libraries
```{r}
library(tidyverse)
library(ranger)
library(randomForest)
library(readr)
library(xgboost)
library(Matrix)

set.seed(345)
```

# Read data
```{r}

sample_submission <- read_csv("sample_submission.csv")
test_set <- read_csv(
  "test.csv",
  col_types = cols(
    id = col_double(),
    
    CERTLEVP  = col_factor(),
    PGMCIPAP  = col_factor(),
    PGM_P034  = col_factor(),
    PGM_P036  = col_factor(),
    HLOSGRDP  = col_factor(),
    PREVLEVP  = col_factor(),
    
    PGM_280A  = col_factor(),
    PGM_280B  = col_factor(),
    PGM_280C  = col_factor(),
    PGM_280F  = col_factor(),
    PGM_P401 = col_factor(),
    
    STULOANS  = col_factor(),
    DBTOTGRD = col_factor(),
    SCHOLARP = col_factor(),
    
    GRADAGEP = col_factor(),
    GENDER2  = col_factor(),
    CTZSHIPP = col_factor(),
    VISBMINP = col_factor(),
    DDIS_FL  = col_factor(),
    
    PAR1GRD = col_factor(),
    PAR2GRD = col_factor(),
    
    BEF_P140 = col_factor(),
    BEF_160  = col_double()
  )
)


train_set <- read_csv(
  "train.csv",
  col_types = cols(
    id            = col_double(),
    
    # Education & Program Features
    CERTLEVP  = col_factor(),
    PGMCIPAP  = col_factor(),
    PGM_P034  = col_factor(),
    PGM_P036  = col_factor(),
    HLOSGRDP  = col_factor(),
    PREVLEVP  = col_factor(),
    
    # Program Experience
    PGM_280A  = col_factor(),
    PGM_280B  = col_factor(),
    PGM_280C  = col_factor(),
    PGM_280F  = col_factor(),
    PGM_P401 = col_factor(),
    
    # Financial
    STULOANS  = col_factor(),
    DBTOTGRD = col_factor(),
    SCHOLARP = col_factor(),
    
    # Demographic
    GRADAGEP = col_factor(),
    GENDER2  = col_factor(),
    CTZSHIPP = col_factor(),
    VISBMINP = col_factor(),
    DDIS_FL  = col_factor(),
    
    # Parental education
    PAR1GRD = col_factor(),
    PAR2GRD = col_factor(),
    
    # Pre-program work
    BEF_P140 = col_factor(),
    BEF_160  = col_double()
  )
)

<<<<<<< HEAD
train_set$overqualified <- factor(train_set$overqualified)
```
#Cleaning block
```{r}
# =========================
# 0) Helpers
# =========================

get_mode <- function(x) {
  x <- x[!is.na(x)]
  if (length(x) == 0) return(NA)
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

missing_rate <- function(x) mean(is.na(x))

impute_with_value <- function(x, value) {
  x[is.na(x)] <- value
  x
}

# =========================
# 1) Recode special missing codes -> NA
#    (DO THIS FIRST)
# =========================

# BEF_160 is numeric with 99 = missing
train_set$BEF_160[train_set$BEF_160 == 99] <- NA
test_set$BEF_160[test_set$BEF_160 == 99]   <- NA

# Categorical recoding
# (treat 6 valid skip + 9 not stated + 99 not stated as NA based on variable)

# CERTLEVP: 9 -> NA
train_set$CERTLEVP[train_set$CERTLEVP == "9"] <- NA
test_set$CERTLEVP[test_set$CERTLEVP == "9"]   <- NA

# PGMCIPAP: 99 -> NA
train_set$PGMCIPAP[train_set$PGMCIPAP == "99"] <- NA
test_set$PGMCIPAP[test_set$PGMCIPAP == "99"]   <- NA

# PGM_P034: 6,9 -> NA
train_set$PGM_P034[train_set$PGM_P034 %in% c("6","9")] <- NA
test_set$PGM_P034[test_set$PGM_P034 %in% c("6","9")]   <- NA

# PGM_P036: 6,9 -> NA
train_set$PGM_P036[train_set$PGM_P036 %in% c("6","9")] <- NA
test_set$PGM_P036[test_set$PGM_P036 %in% c("6","9")]   <- NA

# HLOSGRDP: 9 -> NA
train_set$HLOSGRDP[train_set$HLOSGRDP == "9"] <- NA
test_set$HLOSGRDP[test_set$HLOSGRDP == "9"]   <- NA

# PREVLEVP: 9 -> NA
train_set$PREVLEVP[train_set$PREVLEVP == "9"] <- NA
test_set$PREVLEVP[test_set$PREVLEVP == "9"]   <- NA

# PGM_280A/B/C/F: 6,9 -> NA
for (v in c("PGM_280A","PGM_280B","PGM_280C","PGM_280F")) {
  train_set[[v]][train_set[[v]] %in% c("6","9")] <- NA
  test_set[[v]][test_set[[v]] %in% c("6","9")]   <- NA
}

# PGM_P401: 6,9 -> NA
train_set$PGM_P401[train_set$PGM_P401 %in% c("6","9")] <- NA
test_set$PGM_P401[test_set$PGM_P401 %in% c("6","9")]   <- NA

# STULOANS: 6,9 -> NA
train_set$STULOANS[train_set$STULOANS %in% c("6","9")] <- NA
test_set$STULOANS[test_set$STULOANS %in% c("6","9")]   <- NA

# DBTOTGRD: 6,9 -> NA
train_set$DBTOTGRD[train_set$DBTOTGRD %in% c("6","9")] <- NA
test_set$DBTOTGRD[test_set$DBTOTGRD %in% c("6","9")]   <- NA

# SCHOLARP: 6,9 -> NA
train_set$SCHOLARP[train_set$SCHOLARP %in% c("6","9")] <- NA
test_set$SCHOLARP[test_set$SCHOLARP %in% c("6","9")]   <- NA

# VISBMINP: 9 -> NA
train_set$VISBMINP[train_set$VISBMINP == "9"] <- NA
test_set$VISBMINP[test_set$VISBMINP == "9"]   <- NA

# PAR1GRD / PAR2GRD: 6,9 -> NA
for (v in c("PAR1GRD","PAR2GRD")) {
  train_set[[v]][train_set[[v]] %in% c("6","9")] <- NA
  test_set[[v]][test_set[[v]] %in% c("6","9")]   <- NA
}

# BEF_P140: 9 -> NA
train_set$BEF_P140[train_set$BEF_P140 == "9"] <- NA
test_set$BEF_P140[test_set$BEF_P140 == "9"]   <- NA


# =========================
# 2) Apply your 5% rule:
#    - In TRAIN:
#         <5% NA -> drop rows with NA in that column
#         >=5% NA -> impute (mode / median)
#    - In TEST:
#         never drop rows (must predict all)
#         just impute using TRAIN-derived values
# =========================

threshold <- 0.05

# Identify feature columns (exclude target if present)
target <- "overqualified"

feature_cols <- setdiff(names(train_set), target)

# We'll store imputers learned from train
imputer <- list(mode = list(), median = list())

# (A) First: handle dropping in TRAIN for columns with <5% missing
drop_cols <- c()

for (col in feature_cols) {
  mr <- missing_rate(train_set[[col]])
  if (mr > 0 && mr < threshold) {
    drop_cols <- c(drop_cols, col)
  }
}

# Drop rows in TRAIN that have NA in ANY of the low-missing columns
if (length(drop_cols) > 0) {
  keep_idx <- complete.cases(train_set[, drop_cols, drop = FALSE])
  train_set <- train_set[keep_idx, ]
}

# (B) Second: impute remaining NAs (>=5% columns OR columns that still have NA)
for (col in feature_cols) {

  # skip id from imputation logic (usually no NA anyway, and not a predictor)
  # if you want to keep id as a column but not as a feature later, that's fine.
  if (col == "id") next

  if (!any(is.na(train_set[[col]])) && !any(is.na(test_set[[col]]))) next

  # numeric: BEF_160 median
  if (col == "BEF_160") {
    med <- median(train_set[[col]], na.rm = TRUE)
    imputer$median[[col]] <- med

    train_set[[col]] <- impute_with_value(train_set[[col]], med)
    test_set[[col]]  <- impute_with_value(test_set[[col]],  med)

  } else {
    # categorical: mode from train
    m <- get_mode(train_set[[col]])
    imputer$mode[[col]] <- m

    train_set[[col]] <- impute_with_value(train_set[[col]], m)
    test_set[[col]]  <- impute_with_value(test_set[[col]],  m)

    # make sure both are factors with consistent levels
    # (union of levels from train + test after imputation)
    all_levels <- sort(unique(c(as.character(train_set[[col]]), as.character(test_set[[col]]))))
    train_set[[col]] <- factor(as.character(train_set[[col]]), levels = all_levels)
    test_set[[col]]  <- factor(as.character(test_set[[col]]),  levels = all_levels)
  }
}

# =========================
# 3) Quick checks
# =========================
cat("Remaining NA in TRAIN:", sum(is.na(train_set)), "\n")
cat("Remaining NA in TEST :", sum(is.na(test_set)), "\n")

dim(train_set)
dim(test_set)

```


#Confirm
```{r}
dim(train_set)
dim(test_set)
=======
# Replace NA with 999 (keep factor columns as factors)
replace_na_999 <- function(df) {
  df %>%
    mutate(
      across(where(is.factor), ~ forcats::fct_explicit_na(.x, na_level = "999")),
      across(where(is.character), ~ tidyr::replace_na(.x, "999")),
      across(where(is.numeric), ~ tidyr::replace_na(.x, 999))
    )
}

train_set <- replace_na_999(train_set)
test_set  <- replace_na_999(test_set)

sapply(train_set, function(x) sum(is.na(x)))
levels(train_set$CERTLEVP)

library(forcats)
library(dplyr)

train_set <- train_set %>%
  mutate(across(everything(), ~ as.factor(.))) %>%
  mutate(across(everything(), ~ fct_explicit_na(.x, na_level = "Missing")))

test_set <- test_set %>%
  mutate(across(everything(), ~ as.factor(.))) %>%
  mutate(across(everything(), ~ fct_explicit_na(.x, na_level = "Missing")))

train_set <- train_set %>% select(-id)
test_set  <- test_set  %>% select(-id)

train_set$overqualified <- as.factor(train_set$overqualified)

>>>>>>> dude
```

# Splitting data
```{r}
#n <- nrow(train_set)
#
#prop_train <- 0.75
#prop_test <- 0.25
#
#train_test <- sample(c("Train", "Test"),
#                     size = n, replace = TRUE, 
#                     prob = c(prop_train, prop_test))
#
#training_data <- train_set[train_test == "Train", ]
#testing_data <- train_set[train_test == "Test",]

n <- nrow(train_set)
idx_train <- sample(seq_len(n), size = floor(0.75 * n), replace = FALSE)

training_data <- train_set[idx_train, ]
testing_data  <- train_set[-idx_train, ]

<<<<<<< HEAD
=======
train_test <- sample(c("Train", "Test"),
                     size = n, replace = TRUE, 
                     prob = c(prop_train, prop_test))

iting_data <- train_set[train_test == "Train", ]
testing_data <- train_set[train_test == "Test",]
>>>>>>> dude
```

#Random forest
```{r}
tab <- table(training_data$overqualified)

<<<<<<< HEAD
pro_rf <- randomForest(
  overqualified ~ . - id,
  data = training_data,
  mtry = 8,
  ntree = 1000,
  nodesize = 1,
  importance = TRUE,
  keep.forest = TRUE,
  sampsize = rep(min(tab), 2)
=======
#the one that worked 
set.seed(42)
rf_model <- randomForest(
  overqualified ~ .,
  data = train_set,
  ntree = 700,
  mtry = 9,
  classwt = c("0" = 1, "1" = 2),
  importance = TRUE
>>>>>>> dude
)
rf_model
# OOB predicted probabilities for class "1"
oob_probs <- rf_model$votes[, "1"]

roc_obj_oob <- roc(
  response  = train_set$overqualified,
  predictor = oob_probs,
  levels = c("0","1"),
  direction = "<"
)

auc(roc_obj_oob)

<<<<<<< HEAD
=======

all_mtry <- c(6,7,8,9,10,11)
all_nodesize <- c(1,2,3,5,8,10,15,20)
all_pars <- expand.grid(mtry = all_mtry, nodesize = all_nodesize)
n_pars <- nrow(all_pars)

```
#randome forest 3
```{r}
set.seed(42)

rf_model3 <- randomForest(
  overqualified ~ .,
  data = train_set,
  ntree = 800,
  mtry = 6,
  nodesize = 10,
  classwt = c("0"=1, "1"=2),
  importance = TRUE
)

rf_model3

library(pROC)

oob_probs3 <- rf_model3$votes[, "1"]

roc_obj3 <- roc(
  response  = train_set$overqualified,
  predictor = oob_probs3,
  levels = c("0","1"),
  direction = "<"
)

auc(roc_obj3)

```

# Visuals
```{r}
plot(pro_rf, main = "OOB Error")

plot(pro_rf, xlim = c(50, 1000), ylim = c(0.6, 0.8))

hist(treesize(pro_rf))

>>>>>>> dude
print(pro_rf)
varImpPlot(pro_rf)

```

<<<<<<< HEAD
# XGboost
```{r}

#pro_rf <- randomForest(
#  overqualified ~ . - id,
#  data = training_data,
#  mtry = 8,
#  ntree = 1000,
#  nodesize = 1,
#  importance = TRUE,
#  keep.forest = TRUE,
#  sampsize = rep(min(tab), 2)
#)

# =========================
# XGBoost
# =========================

# Create design matrices (one-hot encoding)
# NOTE: use the SAME formula for both so columns align.
x_train <- sparse.model.matrix(overqualified ~ . - id, data = training_data)[, -1]
x_valid <- sparse.model.matrix(overqualified ~ . - id, data = testing_data)[, -1]

# Labels must be 0/1 numeric
y_train <- as.numeric(as.character(training_data$overqualified))
y_valid <- as.numeric(as.character(testing_data$overqualified))

# DMatrix objects (more efficient)
dtrain <- xgb.DMatrix(data = x_train, label = y_train)
dvalid <- xgb.DMatrix(data = x_valid, label = y_valid)

# Class imbalance handling (similar spirit to your balanced RF)
neg <- sum(y_train == 0)
pos <- sum(y_train == 1)
scale_pos_weight <- neg / pos

params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eval_metric = "logloss",
  eta = 0.05,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 0.8,
  colsample_bytree = 0.8,
  lambda = 1,
  alpha = 0,
  scale_pos_weight = scale_pos_weight
)

set.seed(345)
xgb_fit <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 5000,
  watchlist = list(train = dtrain, valid = dvalid),
  early_stopping_rounds = 50,
  verbose = 1
)

xgb_fit

```

## XGboost validation
```{r}
p_valid <- predict(xgb_fit, dvalid)
pred_valid <- ifelse(p_valid >= 0.5, 1, 0)

conf_mat <- table(True = y_valid, Pred = pred_valid)
conf_mat

acc <- mean(pred_valid == y_valid)
acc
```

#Feature importance plot (nice for your report)
```{r}
imp <- xgb.importance(model = xgb_fit)
head(imp, 15)
xgb.plot.importance(imp, top_n = 20)

```

#Train final XGBoost on full train and create submission
```{r}
# Build full train matrix
x_full <- sparse.model.matrix(overqualified ~ . - id, data = train_set)[, -1]
y_full <- as.numeric(as.character(train_set$overqualified))
dfull <- xgb.DMatrix(data = x_full, label = y_full)

# Build test matrix with matching columns using a dummy target
tmp_test <- test_set
tmp_test$overqualified <- factor(0, levels = levels(train_set$overqualified))
x_test <- sparse.model.matrix(overqualified ~ . - id, data = tmp_test)[, -1]
dtest <- xgb.DMatrix(data = x_test)

neg_full <- sum(y_full == 0)
pos_full <- sum(y_full == 1)
params$scale_pos_weight <- neg_full / pos_full

set.seed(345)
xgb_full <- xgb.train(
  params = params,
  data = dfull,
  nrounds = xgb_fit$best_iteration,
  verbose = 0
)

test_prob <- predict(xgb_full, dtest)
test_pred <- ifelse(test_prob >= 0.5, 1, 0)

submission <- tibble(
  id = test_set$id,
  overqualified = test_pred
)

write_csv(submission, "submission_xgb.csv")
submission |> head()

```
=======
# FIX THIS CODE
```{r}
# ============================================================
# Random Forest Challenger — adapted to YOUR train_set structure
# Target: overqualified (factor with levels "0","1")
# Data already prepared above: train_set (no id), test_set (no id)
# ============================================================

library(dplyr)
library(caret)
library(pROC)
library(randomForest)

# --- Step 0: Safety checks / ensure correct factor levels ---
stopifnot("overqualified" %in% names(train_set))
train_set$overqualified <- factor(train_set$overqualified, levels = c("0","1"))

# --- Step 1: Train/Validation split (stratified) ---
set.seed(445)
idx <- createDataPartition(train_set$overqualified, p = 0.60, list = FALSE)

estimation <- train_set[idx, , drop = FALSE]
validation <- train_set[-idx, , drop = FALSE]

# --- Step 2: Balanced training sample (undersample majority class) ---
set.seed(123)
n_min <- min(table(estimation$overqualified))

estimation_balanced <- estimation %>%
  group_by(overqualified) %>%
  sample_n(size = n_min) %>%
  ungroup()

cat("\nClass proportions (estimation vs balanced):\n")
print(prop.table(table(estimation$overqualified)))
print(prop.table(table(estimation_balanced$overqualified)))

# --- Step 3: Logistic Regression baseline (imbalanced) ---
# Use all predictors (they're mostly factors in your pipeline)

# --- FIX: align factor levels between train (estimation) and newdata (validation/test) ---
align_levels <- function(train_df, new_df, outcome = "overqualified", unknown_level = "Missing") {
  # make sure unknown_level exists in training factors (except outcome)
  for (nm in names(train_df)) {
    if (nm == outcome) next
    if (is.factor(train_df[[nm]])) {
      if (!(unknown_level %in% levels(train_df[[nm]]))) {
        levels(train_df[[nm]]) <- c(levels(train_df[[nm]]), unknown_level)
      }
    }
  }

  # coerce new_df columns to training factor levels; unseen -> unknown_level
  for (nm in intersect(names(train_df), names(new_df))) {
    if (nm == outcome) next
    if (is.factor(train_df[[nm]])) {
      x <- as.character(new_df[[nm]])
      x[is.na(x)] <- unknown_level
      x[!(x %in% levels(train_df[[nm]]))] <- unknown_level
      new_df[[nm]] <- factor(x, levels = levels(train_df[[nm]]))
    }
  }
  new_df
}

# Apply after split:
validation <- align_levels(estimation, validation, outcome = "overqualified", unknown_level = "Missing")

# If you will predict on test_set too:
test_set <- align_levels(estimation, test_set, outcome = "overqualified", unknown_level = "Missing")


# Predicted probability of class "1"

# --- Step 4: Random Forest models ---
# Pick a reasonable default mtry if you don't want to tune
mtry_val <- floor(sqrt(ncol(estimation) - 1))

set.seed(123)
rf_unbal <- randomForest(
  overqualified ~ .,
  data = estimation,
  ntree = 1000,
  mtry = mtry_val,
  importance = TRUE
)

set.seed(123)
rf_bal <- randomForest(
  overqualified ~ .,
  data = estimation_balanced,
  ntree = 1000,
  mtry = mtry_val,
  importance = TRUE
)

# --- Step 5: Probability adjustment for the balanced RF ---
# True and sample prevalence (class "1")
pi_true   <- mean(estimation$overqualified == "1")
pi_sample <- mean(estimation_balanced$overqualified == "1")  # ~0.50

# correction on log-odds
adj_factor <- log((pi_true * (1 - pi_sample)) / (pi_sample * (1 - pi_true)))

# RF predicted probs for class "1"
validation$prob_rf_unbal <- predict(rf_unbal, newdata = validation, type = "prob")[, "1"]

prob_rf_bal_unadj <- predict(rf_bal, newdata = validation, type = "prob")[, "1"]
prob_rf_bal_unadj <- pmin(pmax(prob_rf_bal_unadj, 1e-6), 1 - 1e-6)  # avoid log(0)

logit_bal     <- log(prob_rf_bal_unadj / (1 - prob_rf_bal_unadj))
logit_bal_adj <- logit_bal + adj_factor

validation$prob_rf_bal_adj <- exp(logit_bal_adj) / (1 + exp(logit_bal_adj))

# --- Step 6: Metrics helper ---
get_metrics <- function(prob, truth, cutoff = 0.5) {
  truth <- factor(truth, levels = c("0","1"))
  pred  <- factor(ifelse(prob > cutoff, "1", "0"), levels = c("0","1"))

  cm <- confusionMatrix(pred, truth, positive = "1")
  roc_obj <- roc(truth, prob, levels = c("0","1"), direction = "<")
  auc_val <- as.numeric(auc(roc_obj))

  data.frame(
    Accuracy    = unname(cm$overall["Accuracy"]),
    Sensitivity = unname(cm$byClass["Sensitivity"]),
    Specificity = unname(cm$byClass["Specificity"]),
    AUC         = auc_val
  )
}

# --- Step 7: Compare models ---
m_logit      <- get_metrics(validation$prob_logit,      validation$overqualified)
m_rf_unbal   <- get_metrics(validation$prob_rf_unbal,   validation$overqualified)
m_rf_bal_adj <- get_metrics(validation$prob_rf_bal_adj, validation$overqualified)

compare_metrics <- bind_rows(
  cbind(Model = "Logistic Regression (Imbalanced)", m_logit),
  cbind(Model = "Random Forest (Imbalanced)",       m_rf_unbal),
  cbind(Model = "Random Forest (Balanced Adjusted)",m_rf_bal_adj)
) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

cat("\n=== Model Performance Comparison (Validation) ===\n")
print(compare_metrics)

# --- Step 8: Variable importance plot (balanced RF) ---
varImpPlot(
  rf_bal,
  main = "Variable Importance — RF (Balanced; probabilities adjusted separately)",
  type = 2,
  n.var = 15,
  cex = 0.8
)


```

```{r}
# ============================================================
# Random Forest Challenger — adapted to YOUR train_set structure
# Target: overqualified (factor with levels "0","1")
# ============================================================

library(dplyr)
library(caret)
library(pROC)
library(randomForest)

# --- Step 0: Ensure target levels ---
stopifnot("overqualified" %in% names(train_set))
train_set$overqualified <- factor(train_set$overqualified, levels = c("0","1"))

# --- Step 1: Train/Validation split (stratified) ---
set.seed(445)
idx <- createDataPartition(train_set$overqualified, p = 0.60, list = FALSE)
estimation <- train_set[idx, , drop = FALSE]
validation <- train_set[-idx, , drop = FALSE]

# --- Step 2: Align factor levels (CRITICAL: do this BEFORE fitting models) ---
align_levels <- function(train_df, new_df, outcome = "overqualified", unknown_level = "Missing") {
  # ensure unknown_level exists in training factor levels
  for (nm in names(train_df)) {
    if (nm == outcome) next
    if (is.factor(train_df[[nm]])) {
      if (!(unknown_level %in% levels(train_df[[nm]]))) {
        levels(train_df[[nm]]) <- c(levels(train_df[[nm]]), unknown_level)
      }
    }
  }

  # force new_df factors to use training levels; unseen -> unknown_level
  for (nm in intersect(names(train_df), names(new_df))) {
    if (nm == outcome) next
    if (is.factor(train_df[[nm]])) {
      x <- as.character(new_df[[nm]])
      x[is.na(x)] <- unknown_level
      x[!(x %in% levels(train_df[[nm]]))] <- unknown_level
      new_df[[nm]] <- factor(x, levels = levels(train_df[[nm]]))
    }
  }
  new_df
}

# Make validation/test consistent with estimation
validation <- align_levels(estimation, validation, outcome = "overqualified", unknown_level = "Missing")
test_set   <- align_levels(estimation, test_set,   outcome = "overqualified", unknown_level = "Missing")

# --- Step 3: Balanced training sample (undersample majority class) ---
set.seed(123)
n_min <- min(table(estimation$overqualified))
estimation_balanced <- estimation %>%
  group_by(overqualified) %>%
  sample_n(size = n_min) %>%
  ungroup()

cat("\nClass proportions (estimation vs balanced):\n")
print(prop.table(table(estimation$overqualified)))
print(prop.table(table(estimation_balanced$overqualified)))

# --- Step 4: Logistic Regression baseline (fit AFTER alignment) ---


# --- Step 5: Random Forest models ---
mtry_val <- floor(sqrt(ncol(estimation) - 1))

set.seed(123)
rf_unbal <- randomForest(
  overqualified ~ .,
  data = estimation,
  ntree = 1000,
  mtry = mtry_val,
  importance = TRUE
)

set.seed(123)
rf_bal <- randomForest(
  overqualified ~ .,
  data = estimation_balanced,
  ntree = 1000,
  mtry = mtry_val,
  importance = TRUE
)

# --- Step 6: Probability adjustment for balanced RF ---
pi_true   <- mean(estimation$overqualified == "1")
pi_sample <- mean(estimation_balanced$overqualified == "1")  # ~0.50

adj_factor <- log((pi_true * (1 - pi_sample)) / (pi_sample * (1 - pi_true)))

validation$prob_rf_unbal <- predict(rf_unbal, newdata = validation, type = "prob")[, "1"]

prob_rf_bal_unadj <- predict(rf_bal, newdata = validation, type = "prob")[, "1"]
prob_rf_bal_unadj <- pmin(pmax(prob_rf_bal_unadj, 1e-6), 1 - 1e-6)

logit_bal     <- log(prob_rf_bal_unadj / (1 - prob_rf_bal_unadj))
logit_bal_adj <- logit_bal + adj_factor
validation$prob_rf_bal_adj <- exp(logit_bal_adj) / (1 + exp(logit_bal_adj))

# --- Step 7: Metrics helper ---
get_metrics <- function(prob, truth, cutoff = 0.5) {
  truth <- factor(truth, levels = c("0","1"))
  pred  <- factor(ifelse(prob > cutoff, "1", "0"), levels = c("0","1"))

  cm <- confusionMatrix(pred, truth, positive = "1")
  roc_obj <- roc(truth, prob, levels = c("0","1"), direction = "<")
  auc_val <- as.numeric(auc(roc_obj))

  data.frame(
    Accuracy    = unname(cm$overall["Accuracy"]),
    Sensitivity = unname(cm$byClass["Sensitivity"]),
    Specificity = unname(cm$byClass["Specificity"]),
    AUC         = auc_val
  )
}

# --- Step 8: Compare models ---
compare_metrics <- bind_rows(
  cbind(Model = "Logistic Regression (Imbalanced)", get_metrics(validation$prob_logit,      validation$overqualified)),
  cbind(Model = "Random Forest (Imbalanced)",       get_metrics(validation$prob_rf_unbal,   validation$overqualified)),
  cbind(Model = "Random Forest (Balanced Adjusted)",get_metrics(validation$prob_rf_bal_adj, validation$overqualified))
) %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

cat("\n=== Model Performance Comparison (Validation) ===\n")
print(compare_metrics)

# --- Step 9: Variable importance plot ---
varImpPlot(
  rf_bal,
  main = "Variable Importance — RF (Balanced; probabilities adjusted separately)",
  type = 2,
  n.var = 15,
  cex = 0.8
)

```

```{r}
library(dplyr)
library(caret)
library(pROC)
library(randomForest)

stopifnot("overqualified" %in% names(train_set))
train_set$overqualified <- factor(train_set$overqualified, levels = c("0","1"))

# --- Split ---
set.seed(445)
idx <- createDataPartition(train_set$overqualified, p = 0.60, list = FALSE)
estimation <- train_set[idx, , drop = FALSE]
validation <- train_set[-idx, , drop = FALSE]

# --- Helpers: make characters factors; add Missing to TRAIN; align newdata to TRAIN levels ---
to_factor <- function(df) {
  df %>% mutate(across(where(is.character), as.factor))
}


add_missing_level <- function(df, outcome = "overqualified", unknown_level = "Missing") {
  for (nm in names(df)) {
    if (nm == outcome) next
    if (is.factor(df[[nm]]) && !(unknown_level %in% levels(df[[nm]]))) {
      df[[nm]] <- factor(df[[nm]], levels = c(levels(df[[nm]]), unknown_level))
    }
  }
  df
}

align_to_train <- function(train_df, new_df, outcome = "overqualified", unknown_level = "Missing") {
  for (nm in intersect(names(train_df), names(new_df))) {
    if (nm == outcome) next
    if (is.factor(train_df[[nm]])) {
      x <- as.character(new_df[[nm]])
      x[is.na(x)] <- unknown_level
      x[!(x %in% levels(train_df[[nm]]))] <- unknown_level
      new_df[[nm]] <- factor(x, levels = levels(train_df[[nm]]))
    }
  }
  new_df
}

# --- Apply preprocessing consistently ---
estimation <- to_factor(estimation)
validation <- to_factor(validation)
test_set   <- to_factor(test_set)

estimation <- add_missing_level(estimation, unknown_level = "Missing")
validation <- align_to_train(estimation, validation, unknown_level = "Missing")
test_set   <- align_to_train(estimation, test_set,   unknown_level = "Missing")

# --- Balanced training sample (undersample) ---
set.seed(123)
n_min <- min(table(estimation$overqualified))
estimation_balanced <- estimation %>%
  group_by(overqualified) %>%
  sample_n(size = n_min) %>%
  ungroup()

cat("\nClass proportions (estimation vs balanced):\n")
print(prop.table(table(estimation$overqualified)))
print(prop.table(table(estimation_balanced$overqualified)))

# --- Train RFs ---
mtry_val <- floor(sqrt(ncol(estimation) - 1))

set.seed(123)
rf_unbal <- randomForest(
  overqualified ~ .,
  data = estimation,
  ntree = 1000,
  mtry = mtry_val,
  importance = TRUE
)

set.seed(123)
rf_bal <- randomForest(
  overqualified ~ .,
  data = estimation_balanced,
  ntree = 1000,
  mtry = mtry_val,
  importance = TRUE
)

# --- Balanced probability adjustment ---
pi_true   <- mean(estimation$overqualified == "1")
pi_sample <- mean(estimation_balanced$overqualified == "1")  # 0.5

adj_factor <- log((pi_true * (1 - pi_sample)) / (pi_sample * (1 - pi_true)))

# --- Predict on validation ---
p_unbal <- predict(rf_unbal, newdata = validation, type = "prob")[, "1"]

p_bal <- predict(rf_bal, newdata = validation, type = "prob")[, "1"]
p_bal <- pmin(pmax(p_bal, 1e-6), 1 - 1e-6)
logit_bal_adj <- log(p_bal / (1 - p_bal)) + adj_factor
p_bal_adj <- exp(logit_bal_adj) / (1 + exp(logit_bal_adj))

# --- Metrics (AUC + confusion matrix at cutoff) ---
get_metrics <- function(prob, truth, cutoff = 0.5) {
  truth <- factor(truth, levels = c("0","1"))
  pred  <- factor(ifelse(prob > cutoff, "1", "0"), levels = c("0","1"))

  cm <- confusionMatrix(pred, truth, positive = "1")
  roc_obj <- roc(truth, prob, levels = c("0","1"), direction = "<")
  auc_val <- as.numeric(auc(roc_obj))

  data.frame(
    Accuracy    = unname(cm$overall["Accuracy"]),
    Sensitivity = unname(cm$byClass["Sensitivity"]),
    Specificity = unname(cm$byClass["Specificity"]),
    AUC         = auc_val
  )
}

m_rf_unbal <- get_metrics(p_unbal, validation$overqualified)
m_rf_baladj <- get_metrics(p_bal_adj, validation$overqualified)

compare_metrics <- bind_rows(
  cbind(Model = "RF (Imbalanced)", m_rf_unbal),
  cbind(Model = "RF (Balanced + prior adjusted)", m_rf_baladj)
) %>% mutate(across(where(is.numeric), ~ round(.x, 3)))

cat("\n=== RF Performance Comparison (Validation) ===\n")
print(compare_metrics)

# --- Variable importance plot (balanced RF) ---
varImpPlot(
  rf_bal,
  main = "Variable Importance — RF (Balanced; probs adjusted)",
  type = 2,
  n.var = 15,
  cex = 0.8
)

```

```{r}
class_weights <- c(
  "0" = 1,
  "1" = sum(estimation$overqualified == "0") /
        sum(estimation$overqualified == "1")
)

set.seed(123)

rf_weighted <- randomForest(
  overqualified ~ .,
  data = estimation,
  ntree = 1200,
  mtry = floor(sqrt(ncol(estimation) - 1)),
  nodesize = 5,
  classwt = class_weights,
  importance = TRUE
)
p_weighted <- predict(rf_weighted, validation, type="prob")[,"1"]
m_rf_weighted <- get_metrics(p_weighted, validation$overqualified)
m_rf_weighted

mtry_grid <- seq(2, max(2, floor(sqrt(ncol(estimation) - 1))), by = 2)

auc_results <- data.frame(mtry = integer(), AUC = numeric())

for (m in mtry_grid) {

  rf_tmp <- randomForest(
    overqualified ~ .,
    data = estimation,
    ntree = 600,
    mtry = m,
    nodesize = 5,
    classwt = c(
      "0" = 1,
      "1" = sum(estimation$overqualified == "0") / sum(estimation$overqualified == "1")
    )
  )

  p <- predict(rf_tmp, validation, type = "prob")[, "1"]
  roc_obj <- roc(validation$overqualified, p, levels = c("0","1"), direction = "<")
  auc_val <- as.numeric(auc(roc_obj))   # <- key fix

  auc_results <- rbind(auc_results, data.frame(mtry = m, AUC = auc_val))
}

auc_results[order(auc_results$AUC, decreasing = TRUE), ]
auc_results$AUC <- as.numeric(auc_results$AUC)

```

>>>>>>> dude
